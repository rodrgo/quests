# 017: Build DataFlows with Fluvio

| Prize pool | Start date | End date | Team size | Discord | Workshop |
|  --- | --- | --- | --- | ---  | --- | 
| 1,000 USD | Aug 13, 2024  |  Aug 27, 2024 | 1 | [`#quest-017`](https://discord.gg/quira) | [`#workshop-017`](https://discord.gg/quira) |

## ‚ú® What's special in this Quest?

1. _Turbo loot_ üèéÔ∏è of $10 is available to the first 20 valid submissions ‚ú®
2. The reward distribution will be as follows,
 
| **Rank** | 1st  | 2nd  | 3rd | 4th - 8th |
| -- | -- | -- | -- | -- |
| **Reward** | $300  | $200  | $100 | $50 |

## About Infinyon

InfinyOn is a cloud-based streaming data platform that develops, deploys, and manages real-time data applications. It provides a scalable, secure, and reliable environment to process and analyze streaming data with automated deployment, integration with popular data sources, and enterprise-grade security.

## What is Fluvio

Fluvio is an open-source platform maintained by Infinyon designed for in-motion data. That is, for handling data that is constantly updating or moving, such as live data streams from various sources. It enables users to customise data processing workflows using WebAssembly, allowing for efficient, real-time data services, such as real-time financial trading platforms, interactive live-streaming analytics, or instant notification systems.

Fluvio is built from the ground up as a lean, secure, and simple alternative to Kafka, Flink, and Airflow. The developer experience of using Fluvio Stateful data flow is like ruby on rails for building distributed data pipelines.

## üíª¬†Pre-requisites

1. Make sure you have basic understanding of [event-driven architectures](https://www.youtube.com/watch?v=gOuAqRaDdHA).
2. Make sure you have access to a command line. If you're a Windows user, [install WSL](https://learn.microsoft.com/en-us/windows/wsl/install).
3. Install Fluvio
    1. Open Terminal and run
        
        ```bash
        curl -fsS https://hub.infinyon.cloud/install/install.sh | bash
        ```
        
    2. Add fluvio to your path
        
        ```bash
        echo 'export PATH="${HOME}/.fvm/bin:${HOME}/.fluvio/bin:${PATH}"' >> ~/.bashrc
        ```
        
        ```bash
        echo 'source "${HOME}/.fvm/env"' >> ~/.bashrc
        ```
        
    3. Source the new `.bashrc` file
        
        ```bash
        source ~/.bashrc
        ```
        
    4. You should be able to start the cluster as per https://github.com/infinyon/fluvio
        
        ```bash
        fluvio cluster start
        ```

## üåã¬†The challenge

The challenge is to build DataFlows with Fluvio. There are two types of contributions we anticipate: Operational and AI Data Pipelines.

Operational Data Pipelines are flows which detect anomalies, frauds, and monitor trends and patterns. These use case patterns are used for predictive maintenance, digital twins, industrial automation etc.

AI Data Pipelines are flows include data collection, profiling, pre-processing, running AI model inference workflows, post-processing, monitoring performance of models, drift in data etc. Basically everything outside of training AI models is what part of AI data pipelines.

Some examples of DataFlows you can use are [here](https://github.com/infinyon/stateful-dataflows-examples).

### üêü¬†**Beginner track**

Create a Fluvio data pipeline that collects data from a source API (say, Twitter, Reddit, GitHub), applies transformations and enrichments on the data and then computes trends or writes the standardised data into a database or S3 bucket.

For example, you can use Fluvio‚Äôs Stateful DataFlow in the following way:

- Use the Twitter API to collect data and store it in a Fluvio topic.
- Create a [Fluvio smart module](https://www.fluvio.io/docs/fluvio/how-to/smartmodule-basics) to apply filters on the payload on the connector so that you get the data that you need.
- Use [Fluvio's Stateful DataFlow](https://www.fluvio.io/sdf/) to define the data pipeline:
    - Define operators to filter, map, aggregate tweets
    - Apply map operator to make API callouts to enrich the data
    - Apply AI inferencing into the data flow using model inference APIs
- Display trends in a web interface using apache echarts, openmaps etc. (e.g., localhost).

### üê¨¬†**Medium track**

The challenge for the medium track is to use Fluvio and [Stateful Data Flows](https://www.fluvio.io/sdf/) to build data pipelines and build [connector](https://www.fluvio.io/docs/fluvio/how-to/use-connectors) integrations for the systems you use using Fluvio‚Äôs [connector development kit](https://www.fluvio.io/docs/connectors/local/cdk/introduction). See more here:

- https://github.com/fluvio-connectors
- https://github.com/infinyon/labs-projects

### **üí∞ Claiming the loot**

To claim badges or rewards,

1. Your repo must subscribe to the requirements of any of the tracks above.
2. Your project must have a README. Your README must clearly explain what the project does and how someone can run it or install it.
3. Please create a video (maximum 5 minutes long) to explain your project. Upload the video to YouTube and paste the link in the¬†*YouTube*¬†field when you upload your repo.
4. The content you submit to the quest must be original code.
5. Teams¬†**are not allowed**¬†for this Quest.
6. Each individual can only submit one repo. If you make a mistake, you have to remove the repo and upload a new one.
7. If you build on top of an existing repo, you must be the owner of the repo.
8. You can build on top of a previous Quest repo for this quest.

## üîó¬†Useful links

- [Fluvio's quickstart](https://www.fluvio.io/docs/fluvio/quickstart)
- [Connectors Quickstart](https://www.fluvio.io/docs/connectors/quickstart)
- [Stateful data flows overview](https://www.fluvio.io/sdf)
- [Real-time Streaming Analytics with Fluvio](https://infinyon.com/blog/2024/02/fluvio-deep-causality-rs/)
- [Fluvio examples](https://github.com/deepcausality-rs/fluvio-examples)
